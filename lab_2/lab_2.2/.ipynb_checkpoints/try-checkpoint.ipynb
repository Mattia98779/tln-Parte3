{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatizzazione fatta\n",
      "[(0,\n",
      "  '0.022*\"innocent\" + 0.022*\"warrant\" + 0.019*\"announcement\" + 0.015*\"role\" + '\n",
      "  '0.014*\"convincing\" + 0.014*\"purdue_university\" + 0.013*\"developer\" + '\n",
      "  '0.012*\"domain\" + 0.012*\"computer_network\" + 0.010*\"ins_cwru\"'),\n",
      " (1,\n",
      "  '0.029*\"armenian\" + 0.025*\"kill\" + 0.024*\"turkish\" + 0.023*\"people\" + '\n",
      "  '0.021*\"gun\" + 0.020*\"government\" + 0.019*\"genocide\" + 0.016*\"burn\" + '\n",
      "  '0.015*\"criminal\" + 0.014*\"woman\"'),\n",
      " (2,\n",
      "  '0.036*\"line\" + 0.023*\"new\" + 0.021*\"game\" + 0.020*\"organization\" + '\n",
      "  '0.018*\"nntp_poste\" + 0.017*\"look\" + 0.016*\"drive\" + 0.016*\"host\" + '\n",
      "  '0.015*\"run\" + 0.014*\"m\"'),\n",
      " (3,\n",
      "  '0.023*\"argument\" + 0.018*\"example\" + 0.016*\"evidence\" + 0.015*\"claim\" + '\n",
      "  '0.015*\"truth\" + 0.014*\"true\" + 0.013*\"word\" + 0.013*\"moral\" + '\n",
      "  '0.012*\"therefore\" + 0.012*\"indeed\"'),\n",
      " (4,\n",
      "  '0.038*\"scsi\" + 0.031*\"ide\" + 0.022*\"double\" + 0.021*\"expand\" + '\n",
      "  '0.014*\"cartridge\" + 0.012*\"controller\" + 0.011*\"competition\" + 0.010*\"dd\" + '\n",
      "  '0.003*\"esdi\" + 0.003*\"licensing\"'),\n",
      " (5,\n",
      "  '0.065*\"car\" + 0.034*\"sale\" + 0.031*\"player\" + 0.026*\"door\" + 0.023*\"black\" '\n",
      "  '+ 0.021*\"_\" + 0.019*\"monitor\" + 0.018*\"color\" + 0.017*\"distribution_na\" + '\n",
      "  '0.013*\"sell\"'),\n",
      " (6,\n",
      "  '0.037*\"value\" + 0.025*\"meaning\" + 0.021*\"ignore\" + 0.018*\"warn\" + '\n",
      "  '0.016*\"steal\" + 0.015*\"insert\" + 0.014*\"shaft\" + 0.013*\"bug\" + '\n",
      "  '0.012*\"angle\" + 0.012*\"dream\"'),\n",
      " (7,\n",
      "  '0.073*\"key\" + 0.048*\"tape\" + 0.046*\"morality\" + 0.020*\"portable\" + '\n",
      "  '0.015*\"secure\" + 0.010*\"forsale\" + 0.009*\"secret\" + 0.006*\"duo\" + '\n",
      "  '0.006*\"brand_new\" + 0.005*\"exclusion\"'),\n",
      " (8,\n",
      "  '0.041*\"excellent\" + 0.035*\"health\" + 0.035*\"die\" + 0.034*\"bill\" + '\n",
      "  '0.029*\"disease\" + 0.027*\"patient\" + 0.025*\"scripture\" + 0.019*\"medical\" + '\n",
      "  '0.019*\"test\" + 0.018*\"hell\"'),\n",
      " (9,\n",
      "  '0.047*\"christian\" + 0.039*\"faith\" + 0.032*\"system\" + 0.024*\"encryption\" + '\n",
      "  '0.016*\"private\" + 0.016*\"clipper_chip\" + 0.016*\"clearly\" + '\n",
      "  '0.015*\"government\" + 0.015*\"context\" + 0.015*\"widget\"'),\n",
      " (10,\n",
      "  '0.040*\"box\" + 0.031*\"cross\" + 0.028*\"fax\" + 0.026*\"community\" + '\n",
      "  '0.020*\"catch\" + 0.020*\"owner\" + 0.019*\"server\" + 0.019*\"trust\" + '\n",
      "  '0.017*\"panel\" + 0.014*\"review\"'),\n",
      " (11,\n",
      "  '0.019*\"say\" + 0.019*\"do\" + 0.017*\"write\" + 0.014*\"know\" + 0.013*\"make\" + '\n",
      "  '0.013*\"people\" + 0.012*\"think\" + 0.012*\"go\" + 0.012*\"see\" + '\n",
      "  '0.012*\"article\"'),\n",
      " (12,\n",
      "  '0.017*\"training\" + 0.014*\"train\" + 0.004*\"revolver\" + 0.004*\"maximum\" + '\n",
      "  '0.002*\"inclination\" + 0.001*\"unnecessarily\" + 0.000*\"arm\" + 0.000*\"pilot\" + '\n",
      "  '0.000*\"astronaut_candidate\" + 0.000*\"astronaut\"'),\n",
      " (13,\n",
      "  '0.038*\"graphic\" + 0.027*\"brother\" + 0.027*\"port\" + 0.026*\"convert\" + '\n",
      "  '0.023*\"character\" + 0.019*\"seek\" + 0.018*\"material\" + 0.016*\"font\" + '\n",
      "  '0.014*\"noise\" + 0.014*\"enhance\"'),\n",
      " (14,\n",
      "  '0.036*\"use\" + 0.033*\"line\" + 0.025*\"subject\" + 0.015*\"organization\" + '\n",
      "  '0.014*\"system\" + 0.014*\"problem\" + 0.013*\"nntp_poste\" + 0.012*\"work\" + '\n",
      "  '0.012*\"get\" + 0.012*\"host\"'),\n",
      " (15,\n",
      "  '0.017*\"frame\" + 0.013*\"wiretap\" + 0.012*\"privacy\" + 0.010*\"twin\" + '\n",
      "  '0.009*\"evolve\" + 0.004*\"digital_equipment\" + 0.003*\"ero\" + 0.003*\"atom\" + '\n",
      "  '0.002*\"yr\" + 0.001*\"sfnet\"'),\n",
      " (16,\n",
      "  '0.019*\"team\" + 0.019*\"report\" + 0.013*\"home\" + 0.013*\"face\" + 0.012*\"hit\" + '\n",
      "  '0.012*\"land\" + 0.012*\"miss\" + 0.011*\"kid\" + 0.010*\"nation\" + 0.010*\"play\"'),\n",
      " (17,\n",
      "  '0.025*\"science\" + 0.024*\"father\" + 0.021*\"space\" + 0.019*\"suggest\" + '\n",
      "  '0.019*\"passage\" + 0.018*\"project\" + 0.018*\"doctor\" + 0.017*\"objective\" + '\n",
      "  '0.015*\"purpose\" + 0.013*\"level\"'),\n",
      " (18,\n",
      "  '0.032*\"cd\" + 0.020*\"motorcycle\" + 0.014*\"label\" + 0.005*\"tip\" + '\n",
      "  '0.001*\"fairing\" + 0.000*\"ship\" + 0.000*\"reader\" + 0.000*\"bundle\" + '\n",
      "  '0.000*\"sven\" + 0.000*\"photo\"'),\n",
      " (19,\n",
      "  '0.049*\"family\" + 0.027*\"ok\" + 0.015*\"chemical\" + 0.011*\"bury\" + 0.009*\"mid\" '\n",
      "  '+ 0.007*\"thumb\" + 0.007*\"arbitrary\" + 0.007*\"mirror\" + 0.006*\"nose\" + '\n",
      "  '0.006*\"escalate\"')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis' has no attribute 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8804/4227320776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[0mdoc_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;31m#pyLDAvis.sklearn.prepare(lda_model, corpus, id2word)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyLDAvis' has no attribute 'gensim'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#def make_trigrams(texts):\n",
    "#   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df.head()\n",
    "data = df.content.values.tolist()\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "data_words = list(sent_to_words(data))[:1000]\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "#trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "#trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "print(\"lemmatizzazione fatta\")\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis' has no attribute 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8804/3106915193.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyLDAvis' has no attribute 'sklearn'"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis = pyLDAvis.sklearn.prepare(lda_model, corpus, id2word)\n",
    "pyLDAvis.display(vis)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
